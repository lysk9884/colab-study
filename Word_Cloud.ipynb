{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCbQkFfuXklI"
      },
      "source": [
        "1. 형태소 분석기를 설치합니다. (제일먼저 실행이 되어야함)\n",
        "\n",
        "\n",
        "* 모든 코드는 각 실행단계별로 하나씩 천천히 실행되어야합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "8B3GD9tYXItx"
      },
      "outputs": [],
      "source": [
        "!apt-get -qq update\n",
        "!apt-get -qq install fonts-nanum\n",
        "!apt-get -qq install fonts-noto-cjk\n",
        "!apt-get -qq install mecab libmecab-dev mecab-ipadic-utf8 mecab-ipadic\n",
        "\n",
        "\n",
        "!pip install konlpy pandas simplejson langdetect googletrans==4.0.0-rc1 wordcloud mecab-python3 nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIaOueJEXs_2"
      },
      "source": [
        "2. 단어 분석을 수행할 파일을 업로드합니다 .csv 파일만 허용합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1w0Tg8hkZLDc"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "targetCsv = \"none\";\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "    print('User Uploaded File \"{name}\" with length {length} bytes'.format(name=fn, length=len(uploaded[fn])));\n",
        "    targetCsv = fn;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxSWP8UUZ1L8"
      },
      "source": [
        "3. 업로드된 csv 파일을 읽습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3u__89FqZ6pU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import io\n",
        "\n",
        "df = pd.read_csv(io.StringIO(uploaded[targetCsv].decode('utf-8')))\n",
        "df = df.reset_index()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4NAzLWTciQi"
      },
      "source": [
        "4. 형태소를 분리합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "g-01vEHKcikB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from konlpy.tag import Kkma\n",
        "from re import match, sub\n",
        "from langdetect import detect\n",
        "from googletrans import Translator\n",
        "import MeCab\n",
        "from collections import Counter\n",
        "from wordcloud import WordCloud\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "\n",
        "# NLTK 데이터 다운로드\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "kkma = Kkma()\n",
        "# MeCab 설정 파일 경로 지정\n",
        "mecab = MeCab.Tagger(\"-r /etc/mecabrc\")\n",
        "translator = Translator()\n",
        "\n",
        "korean_nouns = []\n",
        "japanese_nouns = []\n",
        "english_nouns = []\n",
        "\n",
        "def detect_language(text):\n",
        "    lang = detect(text)\n",
        "    if lang in ['ko', 'ja', 'en']:\n",
        "        return lang\n",
        "    return None\n",
        "\n",
        "def is_only_symbols(text):\n",
        "    return not bool(sub(r'[\\W_]+', '', text))\n",
        "\n",
        "def extract_nouns(text, lang):\n",
        "    if lang == 'ko':\n",
        "        return kkma.nouns(text)\n",
        "    elif lang == 'ja':\n",
        "        parsed = mecab.parse(text)\n",
        "        return [line.split('\\t')[0] for line in parsed.splitlines() if '\\t' in line and '名詞' in line.split('\\t')[-1]]\n",
        "    elif lang == 'en':\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        words = word_tokenize(text)\n",
        "        words = [word for word in words if word.isalpha()]\n",
        "        filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "        pos_tags = pos_tag(filtered_words)\n",
        "        return [word for word, pos in pos_tags if pos.startswith('NN')]\n",
        "    else:\n",
        "        return []\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    text = row[1]\n",
        "    if not text or is_only_symbols(text):\n",
        "        continue\n",
        "    lang = detect_language(text)\n",
        "    if not lang:\n",
        "        continue\n",
        "    ex_sent = [text]\n",
        "\n",
        "    for sent in ex_sent:\n",
        "        nouns = extract_nouns(sent, lang)\n",
        "        for noun in nouns:\n",
        "            if len(str(noun)) >= 2 and not match('^[0-9]', noun):\n",
        "                if lang == 'ko':\n",
        "                    korean_nouns.append(noun)\n",
        "                elif lang == 'ja':\n",
        "                    japanese_nouns.append(noun)\n",
        "                elif lang == 'en':\n",
        "                    english_nouns.append(noun)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmkObF2ldEF0"
      },
      "source": [
        "5. 계산및 출력\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "mAZCGzZNdNJ9",
        "outputId": "35863e38-9d89-4c8d-b048-a5c7bf3a51d2"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 단어 빈도 계산\n",
        "korean_word_count = Counter(korean_nouns)\n",
        "japanese_word_count = Counter(japanese_nouns)\n",
        "english_word_count = Counter(english_nouns)\n",
        "\n",
        "# CSV 파일로 저장\n",
        "pd.DataFrame(korean_word_count.items(), columns=[\"단어\", \"수\"]).to_csv('korean_result.csv', index=False)\n",
        "pd.DataFrame(japanese_word_count.items(), columns=[\"단어\", \"수\"]).to_csv('japanese_result.csv', index=False)\n",
        "pd.DataFrame(english_word_count.items(), columns=[\"단어\", \"수\"]).to_csv('english_result.csv', index=False)\n",
        "\n",
        "# 설치한 폰트를 사용할 수 있도록 설정\n",
        "import matplotlib.font_manager as fm\n",
        "\n",
        "# 한글 폰트 설정\n",
        "fe_ko = fm.FontEntry(\n",
        "    fname='/usr/share/fonts/truetype/nanum/NanumGothic.ttf',\n",
        "    name='NanumGothic')\n",
        "fm.fontManager.ttflist.insert(0, fe_ko)\n",
        "\n",
        "# 일본어 폰트 설정\n",
        "fe_ja = fm.FontEntry(\n",
        "    fname='/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc',\n",
        "    name='NotoSansCJK')\n",
        "fm.fontManager.ttflist.insert(0, fe_ja)\n",
        "\n",
        "plt.rcParams.update({'font.size': 18})\n",
        "\n",
        "# 워드 클라우드 생성\n",
        "korean_wc = WordCloud(font_path='/usr/share/fonts/truetype/nanum/NanumGothic.ttf', relative_scaling=.3, background_color='white').generate_from_frequencies(korean_word_count)\n",
        "japanese_wc = WordCloud(font_path='/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc', relative_scaling=.3, background_color='white').generate_from_frequencies(japanese_word_count)\n",
        "english_wc = WordCloud(relative_scaling=.3, background_color='white').generate_from_frequencies(english_word_count)\n",
        "\n",
        "# 한국어 워드 클라우드 시각화\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(korean_wc, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Korean Word Cloud')\n",
        "plt.show()\n",
        "\n",
        "# 일본어 워드 클라우드 시각화\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(japanese_wc, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Japanese Word Cloud')\n",
        "plt.show()\n",
        "\n",
        "# 영어 워드 클라우드 시각화\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(english_wc, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('English Word Cloud')\n",
        "plt.show()\n",
        "\n",
        "# CSV 파일 다운로드\n",
        "files.download('korean_result.csv')\n",
        "files.download('japanese_result.csv')\n",
        "files.download('english_result.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xG9cqM-YdQDn"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
